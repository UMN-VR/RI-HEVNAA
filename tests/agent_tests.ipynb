{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo_path: /Library/Github/RI-HEVNAA\n",
      "Found 34 python documents.\n",
      "Found 5 markdown documents.\n",
      "Found 7 log documents.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "I'm sorry, I can't provide information about \"RI-HEVNAA\" because it's not a recognizable term or keyword. It could be a specific term in a certain field, a code, or a name. Could you please provide more context or check if there are any typos?\u001b[32;1m\u001b[1;3mI'm sorry, I can't provide information about \"RI-HEVNAA\" because it's not a recognizable term or keyword. It could be a specific term in a certain field, a code, or a name. Could you please provide more context or check if there are any typos?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, I can\\'t provide information about \"RI-HEVNAA\" because it\\'s not a recognizable term or keyword. It could be a specific term in a certain field, a code, or a name. Could you please provide more context or check if there are any typos?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents import OpenAIFunctionsAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "import dotenv\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "dotenv.load_dotenv('creds.env')\n",
    "print_debug = False\n",
    "\n",
    "def print_texts(texts):\n",
    "    if not print_debug:\n",
    "        return\n",
    "    print(f\"Found {len(texts)} texts.\")\n",
    "    i=0\n",
    "    for text in texts:\n",
    "        i = i+1\n",
    "        print(f\"{i}:{text.metadata}\")\n",
    "\n",
    "\n",
    "def load_documents(repo_path, type):\n",
    "    loader = GenericLoader.from_filesystem(\n",
    "        repo_path,\n",
    "        glob=f\"**/**/*\",\n",
    "        suffixes=[type],\n",
    "        parser=LanguageParser()\n",
    "    )\n",
    "\n",
    "    new_documents = loader.load()\n",
    "\n",
    "    if not print_debug:\n",
    "        return new_documents\n",
    "\n",
    "    print(f\"Found {len(new_documents)} documents.\")\n",
    "    for new_document in new_documents:\n",
    "        print(f\"Document: {new_document.metadata}\")\n",
    "    \n",
    "    return new_documents\n",
    "\n",
    "\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "repo_path = working_dir\n",
    "\n",
    "print(f\"repo_path: {repo_path}\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "python_documents = load_documents(repo_path, \".py\")\n",
    "\n",
    "markdown_documents = load_documents(repo_path, \".md\")\n",
    "\n",
    "log_documents = load_documents(repo_path, \".log\")\n",
    "\n",
    "print(f\"Found {len(python_documents)} python documents.\")\n",
    "print(f\"Found {len(markdown_documents)} markdown documents.\")\n",
    "print(f\"Found {len(log_documents)} log documents.\")\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, \n",
    "                                                            chunk_size=2000, \n",
    "                                                            chunk_overlap=200)\n",
    "markdown_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.MARKDOWN,\n",
    "                                                                    chunk_size=2000,\n",
    "                                                                    chunk_overlap=200)\n",
    "\n",
    "log_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.MARKDOWN,\n",
    "                                                                    chunk_size=2000,\n",
    "                                                                    chunk_overlap=200)\n",
    "                                                            \n",
    "\n",
    "texts = python_splitter.split_documents(python_documents)\n",
    "\n",
    "\n",
    "texts += markdown_splitter.split_documents(markdown_documents)\n",
    "\n",
    "\n",
    "texts += log_splitter.split_documents(log_documents)\n",
    "print_texts(texts)\n",
    "\n",
    "# db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"mmr\", # Also test \"similarity\"\n",
    "#     search_kwargs={\"k\": 8},\n",
    "# )\n",
    "\n",
    "# import HNSWLib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()]) \n",
    "#memory = ConversationSummaryMemory(llm=llm, chat_memory=chat_history)\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "\n",
    "\n",
    "\n",
    "# define some tools to use in the agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]\n",
    "\n",
    "# Create the prompt for the agent\n",
    "\n",
    "system_message = SystemMessage(content=\"You are very powerful assistant, but bad at calculating lengths of words.\")\n",
    "#prompt = OpenAIFunctionsAgent.create_prompt(system_message=system_message)\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "\n",
    "agent_executor.run(\"tell me about RI-HEVNAA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
