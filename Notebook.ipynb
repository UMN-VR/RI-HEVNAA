{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP-iW0ZVkE5B"
      },
      "source": [
        "\n",
        "# About RI_HEVNAA\n",
        "\n",
        "It is recomended you use Python 3.10 >\n",
        "\n",
        "RI_HEVNAA is open source, with all code available for download, inspection and modification at [https://github.com/UMN-VR/RI_HEVNAA](https://github.com/UMN-VR/RI_HEVNAA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load creds.env file\n",
        "\n",
        "The file 'creds.env' needs to have a line 'OPENAI_API_KEY=...'\n",
        "\n",
        "    True = Loaded .env file, False = fail\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv('creds.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install pip dependencies \n",
        "\n",
        "Should look like this\n",
        "\n",
        "    Requirement already satisfied: openai in /usr/local/lib/python3.10/site-packages (0.28.0)\n",
        "    Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/site-packages (0.4.0)\n",
        "\n",
        "    \n",
        "    ...\n",
        "    \n",
        "\n",
        "    Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/site-packages (0.28.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/site-packages (0.4.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/site-packages (0.4.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/site-packages (0.0.281)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/site-packages (from chromadb) (1.10.12)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.2 in /usr/local/lib/python3.10/site-packages (from chromadb) (0.7.2)\n",
            "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /usr/local/lib/python3.10/site-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/site-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/site-packages (from chromadb) (1.24.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/site-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/site-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/site-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/site-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/site-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/site-packages (from langchain) (0.0.33)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/site-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (4.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken chromadb langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import dotenv\n",
        "import os\n",
        "from langchain.text_splitter import Language\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import LanguageParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load repo_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repo_path: /Library/Github/RI-HEVNAA\n"
          ]
        }
      ],
      "source": [
        "working_dir = os.getcwd()\n",
        "\n",
        "repo_path = working_dir\n",
        "\n",
        "print(f\"repo_path: {repo_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 37 documents.\n"
          ]
        }
      ],
      "source": [
        "# Load\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    repo_path,\n",
        "    glob=\"**/**/*\",\n",
        "    suffixes=[\".py\", \".ipynb\", \".md\", \".log\"],\n",
        "    parser=LanguageParser()\n",
        ")\n",
        "\n",
        "#add \n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Found {len(documents)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "230"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, \n",
        "                                                               chunk_size=2000, \n",
        "                                                               chunk_overlap=200)\n",
        "markdown_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.MARKDOWN,\n",
        "                                                                    chunk_size=2000,\n",
        "                                                                    chunk_overlap=200)\n",
        "texts = python_splitter.split_documents(documents)\n",
        "texts += markdown_splitter.split_documents(documents)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\", # Also test \"similarity\"\n",
        "    search_kwargs={\"k\": 8},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\") \n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = []\n",
        "question = None\n",
        "answer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Print Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(f\"{messages}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Update Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='Tell me about RI-HEVNAA', additional_kwargs={}, example=False), AIMessage(content=\"RI-HEVNAA, which stands for Real-time Interrupt-driven Hemispheric Emulated von Neumann Agent Architecture, is an advanced chatbot architecture designed to replicate the structure and functionality of the human brain. The architecture draws inspiration from principles of biomimicry and von Neumann's computational model, and it provides a comprehensive framework for building sophisticated and human-like chatbot systems.\\n\\nThe RI-HEVNAA architecture comprises several key components. These include SuperUnits, each equipped with an Agent Central Processing Unit (A-CPU) and specialized Accelerators. SuperUnits include the Agent SuperUnit (A_SU), Left Hemisphere SuperUnit (LH_SU), and Right Hemisphere SuperUnit (RH_SU), each dedicated to different tasks similar to the human brain's division of labor.\\n\\nThe architecture also features real-time interaction with hardware components, enabling seamless coordination and control of physical devices. By integrating with hardware modules such as servos, LiDAR modules, and Inertial Measurement Units (IMUs), the architecture can perform actions based on sensor inputs and spatial coordination tasks.\\n\\nRI-HEVNAA is designed to be scalable and extensible, allowing for the seamless addition of new SuperUnits, Accelerators, and Tasks, and integration with external systems and tools to enhance functionality. It also incorporates feedback mechanisms for continuous improvement.\\n\\nThe architecture is showcased in various examples, demonstrating its capabilities in language processing, hardware interaction, and natural language understanding.\", additional_kwargs={}, example=False)]\n"
          ]
        }
      ],
      "source": [
        "if question is None:\n",
        "    print(\"Ask a question first!\") \n",
        "else:\n",
        "    messages.append(HumanMessage(content=question))\n",
        "    messages.append(AIMessage(content=answer))\n",
        "    print(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import (\n",
        "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
        ")\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "combine_docs_chain = StuffDocumentsChain()\n",
        "vectorstore = \n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# This controls how the standalone question is generated.\n",
        "# Should take `chat_history` and `question` as input variables.\n",
        "template = (\n",
        "    \"Combine the chat history and follow up question into \"\n",
        "    \"a standalone question. Chat History: {chat_history}\"\n",
        "    \"Follow up question: {question}\"\n",
        ")\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "llm = OpenAI()\n",
        "question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain = ConversationalRetrievalChain(\n",
        "    combine_docs_chain=combine_docs_chain,\n",
        "    retriever=retriever,\n",
        "    question_generator=question_generator_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I\\'m sorry, but I couldn\\'t find any information on \"RI-HEVNAA\". It\\'s possible that there may be a typo or misunderstanding in the term. Please provide more context or check the spelling so I can give you an accurate response.', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.chat_memory = messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='Tell me about RI-HEVNAA', additional_kwargs={}, example=False), AIMessage(content=\"RI-HEVNAA, which stands for Real-time Interrupt-driven Hemispheric Emulated von Neumann Agent Architecture, is an advanced chatbot architecture designed to replicate the structure and functionality of the human brain. The architecture draws inspiration from principles of biomimicry and von Neumann's computational model, and it provides a comprehensive framework for building sophisticated and human-like chatbot systems.\\n\\nThe RI-HEVNAA architecture comprises several key components. These include SuperUnits, each equipped with an Agent Central Processing Unit (A-CPU) and specialized Accelerators. SuperUnits include the Agent SuperUnit (A_SU), Left Hemisphere SuperUnit (LH_SU), and Right Hemisphere SuperUnit (RH_SU), each dedicated to different tasks similar to the human brain's division of labor.\\n\\nThe architecture also features real-time interaction with hardware components, enabling seamless coordination and control of physical devices. By integrating with hardware modules such as servos, LiDAR modules, and Inertial Measurement Units (IMUs), the architecture can perform actions based on sensor inputs and spatial coordination tasks.\\n\\nRI-HEVNAA is designed to be scalable and extensible, allowing for the seamless addition of new SuperUnits, Accelerators, and Tasks, and integration with external systems and tools to enhance functionality. It also incorporates feedback mechanisms for continuous improvement.\\n\\nThe architecture is showcased in various examples, demonstrating its capabilities in language processing, hardware interaction, and natural language understanding.\", additional_kwargs={}, example=False)]\n"
          ]
        }
      ],
      "source": [
        "# print & render newline characters\n",
        "print(memory.chat_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "Tell me about RI_HEVNAA\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'qa' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAsk a question: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuestion:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mquestion\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m result \u001b[39m=\u001b[39m qa(question)\n\u001b[1;32m      4\u001b[0m answer \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(answer)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'qa' is not defined"
          ]
        }
      ],
      "source": [
        "question = input(\"Ask a question: \")\n",
        "print(f\"Question:\\n{question}\")\n",
        "result = qa(question)\n",
        "answer = result['answer']\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@main: Starting RI-HEVNAA System  with dir: /Library/Github/RI-HEVNAA\n",
            "Deleting logs\n",
            "\n",
            "@RIHEVNAA.init:\n",
            "\n",
            "@BusManager.init:\n",
            "@WebMonitoringProcess.init:\n",
            "@Accelerators.init:\n",
            "@RIHEVNAA.init: DONE: Init Components\n",
            "Initializing Interhemispheric Units\n",
            "@Unit.init: Starting A_HCPU\n",
            "@A_HCPU.link_accelerators:\n",
            "@A_HCPU.init:\n",
            "Loaded sub-pathway: pathways/hello_world/pathway.json\n",
            "Init Control Unit DONE\n",
            "Init Execution Unit DONE\n",
            "Init Memory Unit DONE\n",
            "@Unit.init: Starting R_HCPU\n",
            "@R_HCPU.link_accelerators:\n",
            "@R_HCPU.init:\n",
            "Loaded sub-pathway: pathways/hello_world/pathway.json\n",
            "Init Control Unit DONE\n",
            "Init Execution Unit DONE\n",
            "Init Memory Unit DONE\n",
            "@Unit.init: Starting L_HCPU\n",
            "@L_HCPU.link_accelerators:\n",
            "@L_HCPU.init:\n",
            "Loaded sub-pathway: pathways/hello_world/pathway.json\n",
            "Init Control Unit DONE\n",
            "Init Execution Unit DONE\n",
            "Init Memory Unit DONE\n",
            "\n",
            "@RIHEVNAA.execute: Starting RI-HEVNAA Execution\n",
            "\n",
            "Creating process for bus_manager\n",
            "Creating process for web_monitoring_process\n",
            "Creating process for accelerators\n",
            "Creating process for agent_unit\n",
            "Creating process for right_unit\n",
            "Creating process for left_unit\n",
            "Attempting to start process bus_manager\n",
            "pid: 90201, daemon: False, alive: True, exitcode: None\n",
            "Attempting to start process web_monitoring_process\n",
            "pid: 90202, daemon: False, alive: True, exitcode: None\n",
            "Attempting to start process accelerators\n",
            "pid: 90203, daemon: False, alive: True, exitcode: None\n",
            "Attempting to start process agent_unit\n",
            "pid: 90204, daemon: False, alive: True, exitcode: None\n",
            "Attempting to start process right_unit\n",
            "pid: 90205, daemon: False, alive: True, exitcode: None\n",
            "Attempting to start process left_unit\n",
            "pid: 90206, daemon: False, alive: True, exitcode: None\n",
            "\n",
            "@RIHEVNAA.execute: All processes started successfully, starting execution\n",
            "\n",
            "@BusManager.execute: Start\n",
            "@Accelerators.execute: Start\n",
            "@WebMonitoringProcess.execute: Start\n",
            "@A_HCPU_Unit.execute\n",
            "@A_HCPU.execute:\n",
            "@R_HCPU_Unit.execute\n",
            "@R_HCPU.execute:\n",
            "@L_HCPU_Unit.execute\n",
            "@L_HCPU.execute:\n",
            "^C\n",
            "Process right_unit:\n",
            "Process bus_manager:\n",
            "Process web_monitoring_process:\n",
            "Process left_unit:\n",
            "Process agent_unit:\n",
            "Process accelerators:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Github/RI-HEVNAA/main.py\", line 53, in <module>\n",
            "    asyncio.run(main())\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
            "    return loop.run_until_complete(main)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Library/Github/RI-HEVNAA/main.py\", line 43, in main\n",
            "    await ri_hevnaa.execute()\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 127, in execute\n",
            "    p.join()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 43, in wait\n",
            "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 27, in poll\n",
            "    pid, sts = os.waitpid(self.pid, flag)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 48, in run_coroutine_in_new_process\n",
            "    loop.run_until_complete(coroutine)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Library/Github/RI-HEVNAA/WebMonitoringProcess.py\", line 34, in execute\n",
            "    await self.run(logger)\n",
            "  File \"/Library/Github/RI-HEVNAA/WebMonitoringProcess.py\", line 43, in run\n",
            "    time.sleep(11)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 48, in run_coroutine_in_new_process\n",
            "    loop.run_until_complete(coroutine)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Library/Github/RI-HEVNAA/BusManager.py\", line 36, in execute\n",
            "    await self.run(logger)\n",
            "  File \"/Library/Github/RI-HEVNAA/BusManager.py\", line 44, in run\n",
            "    time.sleep(5)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 48, in run_coroutine_in_new_process\n",
            "    loop.run_until_complete(coroutine)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Github/RI-HEVNAA/Unit.py\", line 26, in execute\n",
            "    await self.unit_HCPU.execute()\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU.py\", line 82, in execute\n",
            "    await self.run()\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU.py\", line 118, in run\n",
            "    self.RAM = await self.memory_unit.run(self.logger, self.RAM, instruction, program, memory, result)\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU_Sub_Components/MemoryUnit.py\", line 16, in run\n",
            "    time.sleep(1)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 48, in run_coroutine_in_new_process\n",
            "    loop.run_until_complete(coroutine)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Library/Github/RI-HEVNAA/Accelerators.py\", line 72, in execute\n",
            "    time.sleep(1)\n",
            "KeyboardInterrupt\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 48, in run_coroutine_in_new_process\n",
            "    loop.run_until_complete(coroutine)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Library/Github/RI-HEVNAA/Unit.py\", line 26, in execute\n",
            "    await self.unit_HCPU.execute()\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU.py\", line 82, in execute\n",
            "    await self.run()\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU.py\", line 118, in run\n",
            "    self.RAM = await self.memory_unit.run(self.logger, self.RAM, instruction, program, memory, result)\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU_Sub_Components/MemoryUnit.py\", line 16, in run\n",
            "    time.sleep(1)\n",
            "KeyboardInterrupt\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Library/Github/RI-HEVNAA/RIHEVNAA.py\", line 48, in run_coroutine_in_new_process\n",
            "    loop.run_until_complete(coroutine)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 629, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Library/Github/RI-HEVNAA/Unit.py\", line 26, in execute\n",
            "    await self.unit_HCPU.execute()\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU.py\", line 82, in execute\n",
            "    await self.run()\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU.py\", line 118, in run\n",
            "    self.RAM = await self.memory_unit.run(self.logger, self.RAM, instruction, program, memory, result)\n",
            "  File \"/Library/Github/RI-HEVNAA/HCPU_Sub_Components/MemoryUnit.py\", line 16, in run\n",
            "    time.sleep(1)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!python3 main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Troubleshooting 1:** \n",
        "\n",
        "\n",
        "**Troubleshooting 2:**\n",
        "See further advice in the FAQ if you run into trouble:\n",
        "https://github.com/UMN-VR/RI-HEVNAA/blob/master/docs/FAQ.md\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbP09OrvRHI2"
      },
      "source": [
        "# Setup and run RI_HEVNAA with Google Colab\n",
        "\n",
        "As many users do not have a powerful enough CPU or GPU installed on their computer, this guide will explain how to use RI_HEVNAA with a free CPU & GPU from google via their google colab service\n",
        "\n",
        "\n",
        "## Optional: [Read papers](https://github.com/UMN-VR/RI-HEVNAA/tree/main/papers)\n",
        "\n",
        "This will help you understand the related concepts, purpose and capabilities of RI_HEVNAA.\n",
        "\n",
        "[Code Llama: Open Foundation Models for Code](https://github.com/UMN-VR/RI-HEVNAA/blob/main/papers/Code%20Llama%20Open%20Foundation%20Models%20for%20Code.pdf)\n",
        "\n",
        "[Consciousness in Artificial Intelligence Insights from the Science of Consciousness](https://github.com/UMN-VR/RI-HEVNAA/blob/main/papers/Consciousness%20in%20Artificial%20Intelligence%20Insights%20from%20the%20Science%20of%20Consciousness.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WObRSiUqVwT8"
      },
      "source": [
        "**Note:** This web page is a type of web application called a jupyter notebook. Jupyter notebooks are open source web applications that allow you to write and run live code.\n",
        "\n",
        "**Note:** I have written either 'Do every time' or 'First time only' to indicate if an instruction is something you only need to do once when first setting up the system or each time you want to run the RI_HEVNAA server.\n",
        "\n",
        "\n",
        "**Note:** Colab can be a bit slow.\n",
        "\n",
        "## Change runtime type. (First time only)\n",
        "\n",
        "1. Select 'Runtime' in the top menu (of this notebook).\n",
        "2. Select 'Change runtime\n",
        "type'\n",
        "3. Select CPU \n",
        "4. Click SAVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3aTezlyP0ge"
      },
      "source": [
        "## Mount drive so data can be shared with client. (Do every time)\n",
        "\n",
        "To run the following block of code. Hover your mouse over it and then click the play icon on the left of it.\n",
        "\n",
        "**Note**: This may take you to another page where you need to authenticate with your google account.\n",
        "\n",
        "If you find it takes ages (over two minutes) without doing anything then click the stop icon and then play again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP0zC9szP7iD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZwAbjcaNeyq"
      },
      "source": [
        "\n",
        "\n",
        "## Clone RI_HEVNAA repo (First time only, or if you need to update to the latest version)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOY8KmAruLrH"
      },
      "source": [
        "This is done in order to download the source code into google drive so we can run the server application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuokLFWiM4xz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "assert os.path.isdir('/content/drive/MyDrive'), 'Are you sure the drive is mounted? See Mount drive step above'\n",
        "!rm -r  /content/drive/'MyDrive'/RI_HEVNAA\n",
        "!pip install imagecodecs # this is required to enable support for certain types of compressed tif images.\n",
        "!git clone https://github.com/UMN-VR/RI_HEVNAA.git /content/drive/'MyDrive'/RI_HEVNAA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzgwzCukanhx"
      },
      "source": [
        "## Prepare working directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDlGlARVb60B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "assert os.path.isdir('/content/drive/MyDrive/RI_HEVNAA'), 'Are you sure the drive is mounted? See Mount drive step above'\n",
        "!mkdir /content/drive/MyDrive/'RI_HEVNAA'/logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPMZMEpMUJvu"
      },
      "source": [
        "## Start server (Do every time)\n",
        "\n",
        "**Note** This cell should keep running until you stop it. Which you can do after you have finished using RI_HEVNAA.\n",
        "\n",
        "**Important** If you are using this Python Notebook on your local machine, run the code in [Local Execution](#local-execution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execute RI_HEVNAA on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc4cnWxPVUvK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "assert os.path.isdir('/content/drive/MyDrive/RI_HEVNAA'), 'Are you sure the drive is mounted? See Mount drive step above'\n",
        "!python /content/drive/MyDrive/RI_HEVNAA/main.py --dir '/content/drive/MyDrive/RI_HEVNAA' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNaqqVUmYda"
      },
      "source": [
        "**Troubleshooting 1:** If running the code above gives the output \"can't open file [..] main.py\" then you probably need to mount google drive. Go back to the instruction _Mount drive so data can be shared with client. (Do every time)_\n",
        "\n",
        "**Troubleshooting 2:** If starting the server stopped working for you then you may need to run the 'Clone RI_HEVNAA repository' above as the code now receives the syncdir as a command line argument but you will need to update to the latest version (by running the clone command) for this to work.\n",
        "\n",
        "**Troubleshooting 3:** If it appears the client is _very_ slow it may be that other files are still syncing. On mac and windows you can inspect the google drive sync activity. It may be that client server interaction is interrupted by large amounts of sync activity from newly added datasets or other files you have in google drive.\n",
        "\n",
        "**Troubleshooting 4:**\n",
        "See further advice in the FAQ if you run into trouble:\n",
        "https://github.com/UMN-VR/RI-HEVNAA/blob/master/docs/FAQ.md\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q91h8jGUM95b"
      },
      "source": [
        "### Limitations of Google Colab\n",
        "\n",
        "In Colab, you get around 12 hours of execution time but the session will disconnect if you are idle for 1 hour, meaning that every 12 hours, Disk, Memory and CPU Cache will get erased.\n",
        "\n",
        "For larger projects, I recommend using your own server or paperspace. Google cloud and amazon web services also have options for renting GPUs. It's also possible to get a paid colab account which apparently increases the reliability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Credits\n",
        "\n",
        "[Abraham George Smith](https://www.researchgate.net/profile/Abraham-Smith-2) -author of Python notebook used as starting point for this interactive tutorial and main developer of the RootPainter software.\n",
        "\n",
        "[R. Ford Denison](https://darwinianagriculture.wordpress.com/) - Testing,  guidance on usability issues and collaboration with AGS on both the introductory and explanatory text."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9RA9wlPLNqvx",
        "h4qb4FZGSUWj",
        "q91h8jGUM95b",
        "vmU6Zj5svOZN"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
